"""
fmEphys/utils/camera.py

Base classes for preprocessing data.

Classes
-------
CameraInput
    Preprocessing input data from a camera.


Written by DMM, 2021
"""


import os
import cv2
import subprocess
import numpy as np
import pandas as pd
import xarray as xr
from tqdm import tqdm
os.environ["DLClight"] = "True"
import deeplabcut

import fmEphys as fme


class Camera(fme.BaseInput):
    """ Base class for camera inputs.

    Parameters
    ----------
    cfg : dict
        Dictionary of options from the config file.
    recording_name : str
        Name of the recording.
    recording_path : str
        Path to the recording directory.
    camname : str
        Name of the camera, e.g. 'REYE' or 'WORLD'.
    
    Methods
    -------
    deinterlace
        Deinterlace videos and shift timestamps to match new video frames.
    flip_headcams
        Flip headcam videos horizontally and/or vertically.
    define_distortion
        Define distortion from checkerboard video.
    undistort
        Remove distortion from worldcam videos.
    auto_contrast
        Automatically adjust the contrast of the eye camera videos.
    batch_dlc_analysis
        Run DLC pose estimation on videos.
    pose_estimation
        Run DLC pose estimation on the eye camera videos.
    open_dlc_h5
        Open the .h5 file generated by DLC.
    open_dlc_h5_multianimal
        Open the .h5 file generated by DLC for multianimal tracking.
    safe_merge
        Merge a list of xr DataArrays even when their lengths do not match.
    gather_camera_files
        Gather all the files for a given camera.
    pack_video_frames
        Pack video frames into an array.
    pack_position_data
        Pack the camera's dlc points and timestamps together in one DataArray.
    split_xyl
        Convert xarray DataArray of DLC x and y positions and likelihood values
        into separate pandas data structures.
    safe_process
        Run the process method while skipping errors.
        
    """


    def __init__(self, cfg, recording_name, recording_path, camname):

        fme.BaseInput.__init__(self, cfg, recording_name, recording_path)
        self.camname = camname


    def deinterlace(self, videos=None, timestamps=None,
                    exp_fps=30, quiet=False):
        """ Deinterlace videos and shift timestamps to match new video frames.

        If videos and timestamps are provided (as lists), only the provided
        filepaths will be processed. If lists are not provided, subdirectories
        will be searched within animal_directory in the options dictionary, config.
        Both videos and timestamps must be provided, for either to be used.

        Videos will also be rotated 180 deg (so that they are flipped in the horizontal
        and vertical directions) if the option is set in the config file.

        Parameters
        ----------
        videos : list
            List of eyecam and/or worldcam videos at 30fps (default is None). If
            the list is None, the subdirectories will be searched for videos.
        timestamps : list
            List of timestamp csv files for each video in videos (default is None).
            If the list is None, the subdirectories will be searched for timestamps.
        exp_fps : int
            Expected framerate of the videos (default is 30 Hz). If a video does not
            match this framerate, it will be skipped (e.g. if it has a frame rate of
            60 fps, it is assumed to have already been deinterlaced).
        quiet : bool
            When True, the function will not print status updates (default is False).
        
        """

        print('Deinterlacing {} video...'.format(self.camname))

        if 'EYE' in self.camname:

            if self.cfg['rotate_eyecam']:
                do_rotation = True
            else:
                do_rotation = False
        
        elif 'WORLD' in self.camname:

            if self.cfg['rotate_worldcam']:
                do_rotation = True
            else:
                do_rotation = False

        # search subdirectories if both lists are not given
        if videos is None or timestamps is None:
            
            videos = fme.find('*{}*{}*.avi'.format(self.recording_name, self.camname),
                                                            self.recording_path)
            timestamps = fme.find('*{}*{}*.csv'.format(self.recording_name, self.camname),
                                                            self.recording_path)
        
        # iterate through each video
        for vid in videos:

            current_path = os.path.split(vid)[0]

            # Make a save path that keeps the subdirectories. Get out a
            # key from the name of the video that will be shared with
            # all other data of this trial.
            
            vid_name = os.path.split(vid)[1]
            key_pieces = vid_name.split('.')[:-1]
            key = '.'.join(key_pieces)
            
            # open the video
            cap = cv2.VideoCapture(vid)
            
            # get some info about the video
            fps = cap.get(cv2.CAP_PROP_FPS) # frame rate

            if fps != exp_fps:
                return

            savepath = os.path.join(current_path, (key + 'deinter.avi'))
        
            if do_rotation:
                vf_val = 'yadif=1:-1:0, vflip, hflip, scale=640:480'
            elif not do_rotation:
                vf_val = 'yadif=1:-1:0, scale=640:480'

            # could add a '-y' after 'ffmpeg' and before ''-i' so that it overwrites
            # an existing file by default
            cmd = ['ffmpeg', '-i', vid, '-vf', vf_val, '-c:v', 'libx264',
                '-preset', 'slow', '-crf', '19', '-c:a', 'aac', '-b:a',
                '256k']

            if self.cfg['allow_avi_overwrite'] is True:
                cmd.extend(['-y'])
            else:
                cmd.extend(['-n'])

            cmd.extend([savepath])
            
            if quiet is True:
                cmd.extend(['-loglevel', 'quiet'])

            subprocess.call(cmd)


    def flip_headcams(self, quiet=True):
        """ Flip headcam videos horizontally and/or vertically.

        This function will flip headcam videos horizontally and/or vertically
        based on the options in the config file. This is only needed for videos
        that need to have their orientation changed but do not need to be
        deinterlaced.

        Parameters
        ----------
        quiet : bool
            When True, the function will not print status updates (default
            is True).
        """

        h = self.cfg['headcams_hflip']
        v = self.cfg['headcams_vflip']

        if h is True and v is True:
            vf_val = 'vflip, hflip'

        elif h is True and v is False:
            vf_val = 'hflip'

        elif h is False and v is True:
            vf_val = 'vflip'

        vid_list = fme.find('*'+self.camname+'.avi', self.recording_path)

        for vid in vid_list:

            vid_name = os.path.split(vid)[1]
            key_pieces = vid_name.split('.')[:-1]
            key = '.'.join(key_pieces)

            savepath = os.path.join(os.path.split(vid)[0], (key + 'deinter.avi'))

            cmd = ['ffmpeg', '-i', vid, '-vf', vf_val, '-c:v',
                'libx264', '-preset', 'slow', '-crf', '19',
                '-c:a', 'aac', '-b:a', '256k']

            if self.cfg['allow_avi_overwrite'] is True:
                cmd.extend(['-y'])
            else:
                cmd.extend(['-n'])

            cmd.extend([savepath])

            if quiet is True:
                cmd.extend(['-loglevel', 'quiet'])

            # Only do the rotation is at least one axis is being flipped
            if h is True or v is True:
                subprocess.call(cmd)


    def define_distortion(self, checkervid='worldcam_checkerboard',
                                mtxkey='worldcam_mtx', boardw=7, boardh=5):
        """ Define distortion from checkerboard videos.

        Requires a previously recorded video of a 7 by 5 checkerboard pattern (printed
        on a piece of paper and attached to a ridgid surface) moving in front of the
        world camera. Many orientations, rotations, distances, and angles should be
        represented. This method will then compute the distortion coefficients for
        the world camera and save them to a file so that future recordings can have
        this distortion removed. The world camera lens distorts the image significantly,
        but this is not a problem for other cameras.

        This is currently written to only be used for the world camera, but could
        be applied to the topdown camera if needed.

        Parameters
        ----------
        checkervid : str
            The key in the config file that points to the video file that will be
            used to define the distortion coefficients (default is
            'worldcam_checkerboard').
        mtxkey : str
            The key in the config file that will be used to save the distortion
            coefficients (default is 'worldcam_mtx').
        boardw : int
            The number of horizontal squares in the checkerboard pattern (default is 7).
        boardh : int
            The number of vertical squares in the checkerboard pattern (default is 5).

        """

        # Arrays to store object points and image points from all the images.
        objpoints = [] # 3d point in real world space
        imgpoints = [] # 2d points in image plane.

        # Termination criteria
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
        
        # Prepare object points
        objp = np.zeros((boardh*boardw,3), np.float32)
        objp[:,:2] = np.mgrid[0:boardw,0:boardh].T.reshape(-1,2)
        
        # Read in file path of video
        calib_vid = cv2.VideoCapture(self.cfg[checkervid])
        
        # Iterate through frames
        print('getting distortion out of each frame')
        
        for step in tqdm(range(0,int(calib_vid.get(cv2.CAP_PROP_FRAME_COUNT)))):
            
            # Open frame
            ret, img = calib_vid.read()
            
            # Make sure the frame is read in correctly
            if not ret:
                break
            
            # Convert to grayscale
            if img.shape[2] > 1:
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            else:
                gray = img
            
            # Find the chess board corners
            ret, corners = cv2.findChessboardCorners(gray, (boardw,boardh), None)
            
            # If found, add object points, image points (after refining them)
            if ret == True:

                objpoints.append(objp)

                corners2 = cv2.cornerSubPix(gray, corners, (11,11), (-1,-1), criteria)
                imgpoints.append(corners)


        # Calibrate the camera (this is a little slow)
        print('calculating calibration correction paramters')
        ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints,
                                                        gray.shape[::-1], None, None)
        
        # Format as .npz and save the file
        savepath = self.cfg[mtxkey]
        np.savez(savepath, mtx=mtx, dist=dist, rvecs=rvecs, tvecs=tvecs)


    def undistort(self, mtxkey='worldcam_mtx', readcamkey='WORLDdeinter',
                    savecamkey='_WORLDcalib.avi', checkervid='worldcam_checkerboard'):
        """ Remove distortion from world camera videos.
        
        This method will remove the distortion from the world camera videos. It
        requires a .npz file to have been previously written by the method of the
        Camera class called define_distortion (above). If this .npz does not exist,
        then this method will call define_distortion to create it.

        Parameters
        ----------
        mtxkey : str
            The key in the config file that points to the .npz file that contains
            the distortion coefficients (default is 'worldcam_mtx').
        readcamkey : str
            The key in the config file that points to the video file that will be
            read in (default is 'WORLDdeinter').
        savecamkey : str
            The new end of the file name that will be used to allow for the written
            .avi to not overwrite the input video (default is '_WORLDcalib.avi').
        checkervid : str
            The key in the config file that points to the video file that will be
            used to define the distortion coefficients (default is 'worldcam_checkerboard').
            This isn't needed unless the distortion coefficients haven't been defined
            yet and the define_distortion method must be called.

        """

        print('Removing worldcam lens distortion...')

        if not os.path.isfile(self.cfg[mtxkey]):
            self.define_distortion(checkervid, mtxkey)

        # load the parameters
        checker_in = np.load(self.cfg[mtxkey])

        # unpack camera properties
        mtx = checker_in['mtx']
        dist = checker_in['dist']
        rvecs = checker_in['rvecs']
        tvecs = checker_in['tvecs']
        
        # iterate through eye videos and save out a copy which has had distortions removed
        
        world_list = fme.find('*{}*{}*.avi'.format(self.recording_name, readcamkey), self.recording_path)
        
        for world_vid in [x for x in world_list if 'plot' not in x and 'calib' not in x]:
            
            print('undistorting '+ world_vid)
            
            if self.cfg['strict_name']:
                savepath = '_'.join(world_vid.split('_')[:-1])+savecamkey
            
            elif not self.cfg['strict_name']:
               
                head, tail = os.path.splitext(world_vid)
                savepath = '_'.join(['_'.join(head.split('_')[:-2]), head.split('_')[-1], head.split('_')[-2]+'calib'])+tail
                print('saving to '+savepath)

            if os.path.isfile(savepath):
                continue
            
            cap = cv2.VideoCapture(world_vid)
            real_fps = cap.get(cv2.CAP_PROP_FPS)
            
            # setup the file writer
            fourcc = cv2.VideoWriter_fourcc(*'XVID')
            out_vid = cv2.VideoWriter(savepath, fourcc, real_fps,
                            (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
                            int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))
            
            # iterate through all frames
            for step in tqdm(range(0,int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))):
                
                # open frame and check that it opens correctly
                ret, frame = cap.read()
                if not ret:
                    break
                
                # run opencv undistortion function
                undist_frame = cv2.undistort(frame, mtx, dist, None, mtx)
                
                # write the frame to the video
                out_vid.write(undist_frame)
            out_vid.release()
            self.calibvid_path = savepath


    def auto_contrast(self):
        """ Automatically adjust the contrast of the eye camera videos.
        
        A new video will be written with a gamma correction applied following
            gamma = log(mid*255) / log(mean)
        This is only able to apply it to the raw eye camera video, which will be
        saved as a new modified video with the '...deinter.avi' ending.
        
        """
        
        if self.cfg['img_correction']['fix_eyecam_contrast']:
            
            input_list = fme.find('*EYE.avi', self.cfg['animal_directory'])
            
            # iterate through input videos
            for video in input_list:
                
                print('Correcting gamma for '+video)
                
                # build the save path
                head, tail = os.path.split(video)
                new_name = os.path.splitext(tail)[0] + 'deinter.avi'
                savepath = os.path.join(head, new_name)

                # write new video with gamma correction
                vid_read = cv2.VideoCapture(video)
                width = int(vid_read.get(cv2.CAP_PROP_FRAME_WIDTH))
                height = int(vid_read.get(cv2.CAP_PROP_FRAME_HEIGHT))

                fourcc = cv2.VideoWriter_fourcc(*'XVID')
                out_vid = cv2.VideoWriter(savepath, fourcc, 60.0, (width, height))
                num_frames = int(vid_read.get(cv2.CAP_PROP_FRAME_COUNT))
                
                print('num_frames', num_frames)
                # iterate through frames, find ideal gamma, apply, write frame
                for step in tqdm(range(0,num_frames)):
                    
                    ret, frame = vid_read.read()
                    
                    if not ret:
                        break
                    
                    # convert img to gray
                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                    
                    # compute gamma = log(mid*255)/log(mean)
                    mid = 0.5
                    mean = np.mean(gray)
                    gamma = np.log(mid*255)/np.log(mean)
                    
                    # do gamma correction
                    img_gamma1 = np.power(frame, gamma).clip(0,255).astype(np.uint8)
                    
                    # write frame
                    out_vid.write(img_gamma1)

                out_vid.release()


    def batch_dlc_analysis(self, videos, project_cfg):
        """ Run DLC pose estimation on videos.

        Parameters
        ----------
        videos : str or list
            The path to the video file(s) to be analyzed.
        project_cfg : str
            The path to the project config file.

        """
        
        if isinstance(videos, str):
            videos = [videos]
        
        for vid in videos:
            
            if self.cfg['DLC_crop'] is True:
                deeplabcut.cropimagesandlabels(project_cfg,
                                                size=(400, 400),
                                                userfeedback=False)
            
            deeplabcut.analyze_videos(project_cfg, [vid])
            
            if self.cfg['DLC_filt'] is True:

                deeplabcut.filterpredictions(project_cfg, vid)


    def pose_estimation(self):
        """ Run DLC pose estimation on the eye camera videos.
        """
        
        if self.camname in self.cfg['dlc_projects'].keys():
            cam_project = self.cfg['dlc_projects'][self.camname]
        else:
            cam_project = None

        if cam_project != '' and cam_project != 'None' and cam_project != None:
            
            # if it's one of the cameras that needs to needs to be deinterlaced first, make sure and read in the deinterlaced 
            if self.camname=='REYE' or self.camname=='LEYE':
                
                # find all the videos in the data directory that are from the current camera and are deinterlaced
                if self.cfg['strict_name'] is True:
                    vids_this_cam = fme.find('*'+self.camname+'*deinter.avi',  self.recording_path)
                
                elif self.cfg['strict_name'] is False:
                    vids_this_cam = fme.find('*'+self.camname+'*.avi', self.recording_path)
                
                # remove unflipped videos generated during jumping analysis
                bad_vids = fme.find('*'+self.camname+'*unflipped*.avi', self.recording_path)
                
                for x in bad_vids:
                    if x in vids_this_cam:
                        vids_this_cam.remove(x)
                ir_vids = fme.find('*IR*.avi', self.recording_path)
                
                for x in ir_vids:
                    if x in vids_this_cam:
                        vids_this_cam.remove(x)
                
                # warning for user if no videos found
                if len(vids_this_cam) == 0:
                    print('no ' + self.camname + ' videos found -- maybe the videos are not deinterlaced yet?')
            
            else:
                
                # find all the videos for camera types that don't neeed to be deinterlaced
                if self.cfg['strict_name'] is True:
                    vids_this_cam = fme.find('*'+self.camname+'*.avi', self.recording_path)
                
                elif self.cfg['strict_name'] is False:
                    vids_this_cam = fme.find('*'+self.camname+'*.avi', self.recording_path)
            
            # analyze the videos with DeepLabCut
            # this gives the function a list of files that it will iterate over with the same DLC config file
            vids2run = [vid for vid in vids_this_cam if 'plot' not in vid]
            self.batch_dlc_analysis(vids2run, cam_project)


    def open_dlc_h5(self, h5key=None):
        """ Open the .h5 file generated by DLC.

        Parameters
        ----------
        h5key : str
            The key to the .h5 file. Default is None.

        """
        
        if h5key is None:
            # read the .hf file when there is no key
            pts = pd.read_hdf(self.dlc_path)
        
        else:
            # read in .h5 file when there is a key set in corral_files.py
            pts = pd.read_hdf(self.dlc_path, key=h5key)
        
        # organize columns
        pts.columns = [' '.join(col[:][1:3]).strip() for col in pts.columns.values]
        
        pts = pts.rename(columns={pts.columns[n]: pts.columns[n].replace(' ', '_') for n in range(len(pts.columns))})
        pt_loc_names = pts.columns.values
        return pts, pt_loc_names


    def open_dlc_h5_multianimal(self):
        """ Open the .h5 file generated by DLC for multianimal tracking.
        """
        
        pts = pd.read_hdf(self.dlc_path)
        # flatten columns from MultiIndex 
        pts.columns = ['_'.join(col[:][1:]).strip() for col in pts.columns.values]
        return pts


    def safe_merge(self, obj_list, dim_name='frame'):
        """ Merge a list of xr DataArrays even when their lengths do not match.

        Parameters
        ----------
        obj_list : list
            A list of xr DataArrays to be merged.
        dim_name : str
            The name of the dimension to be merged. Default is 'frame'.
        
        Returns
        -------
        out_objs : list
            A list of xr DataArrays that have been merged.

        """
        
        max_lens = []
        
        # iterate through objects
        for obj in obj_list:
            
            # get the sizes of the dim, dim_name
            max_lens.append(dict(obj.frame.sizes)[dim_name])
        
        # get the smallest of the object's length's
        set_len = np.min(max_lens)
        
        # shorten everything to the shortest length found
        out_objs = []
        
        for obj in obj_list:
            
            # get the length of the current object
            obj_len = dict(obj.frame.sizes)[dim_name]
            
            # if the size of dim is longer
            if obj_len > set_len:
                
                # how much does it need to be shortened by?
                diff = obj_len - set_len
                
                # what indeces should be kept?
                good_inds = range(0,obj_len-diff)
                
                # index to remove what would be jagged ends
                obj = obj.sel(frame=good_inds)
                
                # add to the list of objects to merge
                out_objs.append(obj)
            
            # if it is the smallest length or all objects have the same length
            else:
                
                # just append it to the list of objects to merge
                out_objs.append(obj)
        
        # do the merge with the lengths all matching along provided dimension
        self.data = xr.merge(out_objs)

        return self.data


    def gather_camera_files(self):
        """ Gather all the files for a given camera.
        """
        
        if self.camname.lower() != 'world':
            
            # get dlc h5 path
            h5_paths = [x for x in fme.find('*{}*.h5'.format(self.recording_name), self.recording_path) if x != []]
            h5_paths = [x for x in h5_paths if 'DLC' in x]
            self.dlc_path = next(path for path in h5_paths if self.camname in path)
        
        elif self.camname.lower() == 'world': # worldcam will not have h5 files
            self.dlc_path = None
        
        # get avi video and timestamps
        if 'eye' in self.camname.lower() or 'world' == self.camname.lower():
            
            if self.cfg['strict_name']:
                # video
                if 'eye' in self.camname.lower():
                    vidsearchkey = 'deinter'
                elif 'world' in self.camname.lower():
                    vidsearchkey = 'calib'
                
                avi_paths = [x for x in fme.find(('{}*.avi'.format(self.recording_name)), self.recording_path) if x != []]
                self.video_path = next(path for path in avi_paths if self.camname in path and vidsearchkey in path and 'plot' not in path)
                
                # timestamps
                csv_paths = [x for x in fme.find(('{}*BonsaiTS*.csv'.format(self.recording_name)), self.recording_path) if x != []]
                self.timestamp_path = next(i for i in csv_paths if self.camname in i and 'formatted' not in i)
            
            elif not self.cfg['strict_name']:
                # video
                avi_paths = [x for x in fme.find(('*.avi'), self.recording_path) if x != []]
                self.video_path = next(path for path in avi_paths if self.camname in path and 'plot' not in path)
                # timestamps
                csv_paths = [x for x in fme.find(('*BonsaiTS*.csv'), self.recording_path) if x != []]
                if csv_paths != []:
                    self.timestamp_path = next(i for i in csv_paths if self.camname)
                else:
                    self.timestamp_path = None
        
        # all other cameras (i.e. topcam and sidecam)
        else:
            avi_paths = [x for x in fme.find(('*.avi'), self.recording_path) if x != []]
            self.video_path = next(path for path in avi_paths if self.camname in path and 'plot' not in path and 'speed_yaw' not in path)
            csv_paths = [x for x in fme.find(('*BonsaiTS*.csv'), self.recording_path) if x != []]
            self.timestamp_path = next(i for i in csv_paths if self.camname in i)

       
        _prnt_stmnt = ('Reading camera files for {} in recording {}\n'
                       'Found:\n'
                       'video = {}\n'
                       '\ntimestamps = {}\n'
                       'deeplabcut = {}\n'.format(self.camname, self.recording_name,
                                                  self.video_path, self.timestamp_path,
                                                  self.dlc_path))
        print(_prnt_stmnt)


    def pack_video_frames(self, usexr=True, dwnsmpl=None):
        """ Pack video frames into an array.

        Parameters
        ----------
        usexr : bool
            Whether to use xarray or not. Default is True.
        dwnsmpl : float
            How much to downsample the video frames. Default is None.

        Returns
        -------
        all_frames : np.ndarray
            Array of video frames. Only returned if usexr is False,
            otherwise returns None.

        """

        print('Packing video frames into array...')
        
        if dwnsmpl is None:
            dwnsmpl = self.cfg['video_dwnsmpl']
        
        # open the .avi file
        vidread = cv2.VideoCapture(self.video_path)
        
        # empty array that is the target shape
        # should be number of frames x downsampled height x downsampled width
        all_frames = np.empty([int(vidread.get(cv2.CAP_PROP_FRAME_COUNT)),
                            int(vidread.get(cv2.CAP_PROP_FRAME_HEIGHT)*dwnsmpl),
                            int(vidread.get(cv2.CAP_PROP_FRAME_WIDTH)*dwnsmpl)], dtype=np.uint8)
        
        # iterate through each frame
        for frame_num in tqdm(range(0,int(vidread.get(cv2.CAP_PROP_FRAME_COUNT)))):
            
            # read the frame in and make sure it is read in correctly
            ret, frame = vidread.read()
            if not ret:
                break
            
            # convert to grayyscale
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # downsample the frame by an amount specified in the config file
            sframe = cv2.resize(frame, (0,0),
                                fx=dwnsmpl, fy=dwnsmpl,
                                interpolation=cv2.INTER_NEAREST)
            
            # add the downsampled frame to all_frames as int8
            all_frames[frame_num,:,:] = sframe.astype(np.int8)
        
        if not usexr:
            return all_frames
        
        # store the combined video frames in an xarray
        formatted_frames = xr.DataArray(all_frames.astype(np.int8), dims=['frame', 'height', 'width'])
        
        # label frame numbers in the xarray
        formatted_frames.assign_coords({'frame':range(0,len(formatted_frames))})
        # delete all frames, since it's somewhat large in memory
        del all_frames
        self.xrframes = formatted_frames


    def pack_position_data(self):
        """ Pack the camera's dlc points and timestamps together in one DataArray.
        """

        # check that pt_path exists
        if self.dlc_path is not None and self.dlc_path != [] and self.timestamp_path is not None:
            
            # open multianimal project with a different function than single animal h5 files
            if 'TOP' in self.camname and self.cfg['DLC_topMA'] is True:
                
                # add a step to convert pickle files here?
                pts = self.open_dlc_h5_multianimal()
            
            # otherwise, use regular h5 file read-in
            else:
                pts, self.pt_names = self.open_dlc_h5()
            
            # read time file, pass length of points so that it will know if that length matches the length of the timestamps
            # if they don't match because time was not interpolated to match deinterlacing, the timestamps will be interpolated
            time = self.read_timestamp_file(len(pts))
            
            # label dimensions of the points dataarray
            xrpts = xr.DataArray(pts, dims=['frame', 'point_loc'])
            
            # label the camera view
            xrpts.name = self.camname
            
            # assign timestamps as a coordinate to the 
            try:
                xrpts = xrpts.assign_coords(timestamps=('frame', time[1:])) # indexing [1:] into time because first row is the empty header, 0
            
            # correcting for issue caused by small differences in number of frames
            except ValueError:
                
                diff = len(time[1:]) - len(xrpts['frame'])
                
                if diff > 0: # time is longer
                    diff = abs(diff)
                    new_time = time.copy()
                    new_time = new_time[0:-diff]
                    xrpts = xrpts.assign_coords(timestamps=('frame', new_time[1:]))
                
                elif diff < 0: # frame is longer
                    diff = abs(diff)
                    timestep = time[1] - time[0]
                    new_time = time.copy()
                    for i in range(1,diff+1):
                        last_value = new_time[-1] + timestep
                        new_time = np.append(new_time, pd.Series(last_value))
                    xrpts = xrpts.assign_coords(timestamps=('frame', new_time[1:]))
                
                # equal (probably won't happen because ValueError should have been
                # caused by unequal lengths)
                else:
                    xrpts = xrpts.assign_coords(timestamps=('frame', time[1:]))
        
        # pt_path will have no data in it for world cam data, so it will make an
        # xarray with just timestamps
        elif self.dlc_path is None or self.dlc_path == [] and self.timestamp_path is not None:
            
            if self.timestamp_path is not None and self.timestamp_path != []:
                # read in the time
                if 'formatted' in self.timestamp_path:
                    time = self.read_timestamp_file()
                else:
                    time = self.read_timestamp_file(force_timestamp_shift=True)
                # setup frame indices
                xrpts = xr.DataArray(np.zeros([len(time)-1]), dims=['frame'])
                # assign frame coordinates, then timestamps
                xrpts = xrpts.assign_coords({'frame':range(0,len(xrpts))})
                xrpts = xrpts.assign_coords(timestamps=('frame', time[1:]))
            
            elif self.timestamp_path is None or self.timestamp_path == []:
                xrpts = None

        # if timestamps are missing, still read in and format as xarray
        elif self.dlc_path is not None and self.dlc_path != [] and self.timestamp_path is None:
            
            # open multianimal project with a different function than single animal h5 files
            if 'TOP' in self.camname and self.cfg['DLC_topMA'] is True:
                # add a step to convert pickle files here?
                pts = self.open_dlc_h5_multianimal()
            
            # otherwise, use regular h5 file read-in
            else:
                pts, self.pt_names = self.open_dlc_h5()
            # label dimensions of the points dataarray
            xrpts = xr.DataArray(pts, dims=['frame', 'point_loc'])
            # label the camera view
            xrpts.name = self.camname

        self.xrpts = xrpts


    def split_xyl(self):
        """ Convert xarray DataArray of DLC x and y positions
        and likelihood values into separate pandas data structures.

        """

        data = self.xrpts
        names = list(data['point_loc'].values)
        thresh = self.cfg['Lthresh']

        x_locs = []
        y_locs = []
        likeli_locs = []
        
        # seperate the lists of point names into x, y, and likelihood
        for loc_num in range(0, len(names)):
            loc = names[loc_num]
            if '_x' in loc:
                x_locs.append(loc)
            elif '_y' in loc:
                y_locs.append(loc)
            elif 'likeli' in loc:
                likeli_locs.append(loc)
        
        # get the xarray, split up into x, y,and likelihood
        for loc_num in range(0, len(likeli_locs)):
            pt_loc = likeli_locs[loc_num]
            if loc_num == 0:
                likeli_pts = data.sel(point_loc=pt_loc)
            elif loc_num > 0:
                likeli_pts = xr.concat([likeli_pts, data.sel(point_loc=pt_loc)],
                                       dim='point_loc', fill_value=np.nan)
        
        for loc_num in range(0, len(x_locs)):
            pt_loc = x_locs[loc_num]
            # threshold from likelihood
            data.sel(point_loc=pt_loc)[data.sel(point_loc=pt_loc) < thresh] = np.nan
            if loc_num == 0:
                x_pts = data.sel(point_loc=pt_loc)
            elif loc_num > 0:
                x_pts = xr.concat([x_pts, data.sel(point_loc=pt_loc)],
                                  dim='point_loc', fill_value=np.nan)
        
        for loc_num in range(0, len(y_locs)):
            pt_loc = y_locs[loc_num]
            # threshold from likelihood
            data.sel(point_loc=pt_loc)[data.sel(point_loc=pt_loc) < thresh] = np.nan
            if loc_num == 0:
                y_pts = data.sel(point_loc=pt_loc)
            elif loc_num > 0:
                y_pts = xr.concat([y_pts, data.sel(point_loc=pt_loc)],
                                  dim='point_loc', fill_value=np.nan)
        
        x_pts = xr.DataArray.squeeze(x_pts)
        y_pts = xr.DataArray.squeeze(y_pts)
        likeli_pts = xr.DataArray.squeeze(likeli_pts)
        
        # convert to dataframe, transpose so points are columns
        x_vals = xr.DataArray.to_pandas(x_pts).T
        y_vals = xr.DataArray.to_pandas(y_pts).T
        likeli_pts = xr.DataArray.to_pandas(likeli_pts).T
        
        return x_vals, y_vals, likeli_pts


    def safe_process(self, show=False):
        """ Run the process method while skipping errors.

        Errors are printed in the terminal, but the process
        method will continue to run on the next file without
        breaking.

        Parameters
        ----------
        show : bool
            Whether to print errors to the terminal.

        """

        try:
            self.process()

        except Exception as e:
            if show:
                print(e)
            else:
                pass

