# Video Preprocessing -- User Guide

## Use
### Using a .json for individual preprocessing

1. Write a .json config file, named `preprocessing_config.json` (using options below) and save it to the animal directory.
    * `data_path`: path to the parent directory of data, should be the animal directory
    * `steps_to_run`: a dictionary of steps to analysis and whether or not to run those steps as `True`/`False` values. `deinter` deinterlaces all interlaced videos in the recording directories. `get_cam_calibration_params` uses the .checkerboard avi video paths in the `calibration` dictionary in the config file to find distortions in the camera. This will save out the paramters as a .npz file to the path named in that same `calibration` dictionary below. This is done for top and world cameras. The user doesn't need to run `get_cam_calibration_params` every time that the pipeline is run, since the paramters can be reused for a camera. `undistort recordings` makes use of the .npz file of distortion paramters and corrects distortions in world and top videos.
    * `cams`: a dictionary of camera views in the experiments and the .yaml DeepLabCut config file paths to be used to analyze the videos of each camera type; the capitalization of the camera name (i.e. LEYE vs LEye) must match the capitalization of the files exactly. Any combination of capitalization should be fine. Topdown cameras, eye, world, and side cameras are all acceptable.
    * `calibration`: a dictionary with information about calibrating world and topdown cameras, but also about tracking the IR LEDs for use in calibration. `eye_LED_config` should be the DeepLabCut .yaml config file path to track the reflection of the IR LED on the mouse pupil. `world_LED_config` is the .yaml path for tracking the IR LED in the worldcam. `world_checker_vid` should be the worldcam checkerboard .avi video that will be used to generate the .npz of calibration paramters in the event that `get_cam_calibration_params` is `True` in `steps_to_run`. `world_checker_npz` is treated as the save path for the .npz of worldcam calibration paramters if `get_cam_calibration_params` is `True`, but the path to read the .npz from when `undistort_recording` is True. It will overwrite existing .npz files. `top_checker_vid` and `top_checker_npz` ar ethe equivilent paths for the topdown camera.
    * `LED_dir_name` is the name of the directory in which the IR LED recordings are saved and should be read from. If the user is not running `addtl_params`, this will be ignored.
    * `flip_eye_during_deinter`: whether or not to flip the eye video verticlly during deinterlacing (eye video must be right-side-up, or DLC tracking will work poorly)
    * `flip_world_during_deinter`: whether or not to flip the world video vertically during deinterlacing (world video should be right-side-up)
    * `crop_for_dlc`: whether or not to crop the videos down for DLC analysis
    * `multianimal_TOP`: whether or not the already trained TOP network to be used is a multianimal project (there are different functions to read in the TOP network dependeing on how the data are structured wihtin the .h5 output from DLC)
    * `lik_thresh`: threshold to set for pose likelihood
    * `lik_thresh_strict`: this is only used for IR LED tracking at the moment. It's a more strict and less inclusive threshold for DLC likleihood values.
    * `has_ephys`: whether or not to run individual ephys analysis for a recording (ephys should be split out and processed using the script `format_multi_ephys.py`, and not processed alongside videos, so `has_ephys` should always be `False`)
    * `cricket`: whether or not there is a cricket to track in the experiments
    * `tear`: whether or not the outer point and tear duct of the eye was labeled in the eye videos
    * `pxl_thresh`: the maximum acceptable number of pixels for radius of the pupil
    * `ell_thresh`: the maximum ratio of ellipse shortaxis to longaxis during ellipse fit of pupil
    * `eye_dist_thresh_cm`: max. acceptable distance from mean position of an eye point that any frame's position for that point can be (in cm)
    * `eyecam_pxl_per_cm`: scale factor for camera from pixels to cm on eye
    * `save_avi_vids`: whether or not to save out videos with parameters plotted on them
    * `num_save_frames`: number of video frames to write to file with parameters plotted on them
    * `save_nc_vids`: whether or not to save compressed videos into .nc files along with the data
    * `save_figs`: whether or not to save out figures
    * `use_BonsaiTS`: whether to use Bonsai timestamps for Flir timestamps, where `True` would have it use Bonsai timestamps
    * `range_radius`: the threshold to set for range in radius of the pupil to be used to find pupil rotation
    * `world_interp_method`: the interpolation method to use for interpolating over eye timestmaps with world timestamps
    * `num_ellipse_pts_needed`: the number of 'good' eye points required before an ellipse fit will be done on a frame
    * `dwnsmpl`: factor by which to downsample videos before frames are added to an xarray data structure
    * `ephys_sample_rate`: sample rate of ephys aquisition
    * `optical_mouse_screen_center`: dictionary of x and y centers of screen that the optical mouse is reset to
    * `optical_mouse_pix2cm`: scale factor for optical mouse pixels to cm
    * `optical_mouse_sample_rate_ms`: optical mouse sample rate in ms
    * `imu_sample_rate`: sample rate for IMU
    * `imu_downsample` factor to downsample IMU data by
    * `run_pupil_rotation`: whether or not to analyze eye videos for pupil rotation
    * `run_top_angles`: whether or not to get TOP head and body angles; turn off when tracking quality is poor
    * `run_with_form_time`: this changes how strictly nomenclature must be followed for post-deinterlacing files; if files are *not* going to have 'deinter'/'calib' or 'formatted' in the names of .avi and .csv files, then `run_with_form_time` should be `False`, but most often, if you're running the pipeline start to finish, it should be `True`.
2. Run `python manual_preprocessing.py --config_path /path/to/animal/directory/preprocessing_config.json`

### Using the GUI for individual preprocessing

1. Run `python -m preprocessing` to launch the GUI after actvating the appropriate conda environment.
2. The GUI will open on a 'Welcome' tab. If you have already manually edited a .json config file, or you previously made a config file that you're ready to use without making changes, click the browse button on this tab adn select the .json file. Skip to step 10, moving past all of the other tabs which will now be impossible to select. Skip this step if you want to write a config file.
3. Select the animal path in the next tab. This is where data will be read in from where outputs will be saved, and where the config file will be written to.
4. On the next tab, check the box if you want to deinterlace eye and world videos. Once checked, two more boxes will appear below. When selected, these checkboxes will flip the eye and world cameras verticlly and horizontally.
5. **If there are no existing .npz files:** You'll have to create .npz files using checkerboard videos of the top and world cameras. Check the box to 'get paramters from checkerboard videos.' Then, select the paths to (1) the top camera checkerboard video, (2) the world camera checkerboard video, and (3) the directory into which you'd like to save the calibration parameters. You'll also have to choose names for the top and world .npz files which will be saved in the directory in item (3) above. There are default names already in those fields. If you want to check anything, you can print out the default options that will be used if you enter nothing (these are read in from the default config in `/FreelyMovingEphys/example_configs/preprocessing_config.json`), and you can print out what things are currently set to, which will update as browse buttons are clicked. These will be printed in the terminal window that is running the GUI. Criticlly, you will also need to check the very last checkbox in this tab, 'use parameters saved out during this session,' so that the pipeline knows to use the save paths entered above as the read paths as well. This will allow videos to be undistorted using the paramters right after those parameters are written to file. **If there are existing .npz files already written:** In this case, you just need to click 'undistort top and world videos using existing calibration parameters,' and pick (1) the path to the top .npz and (2) the path to teh world .npz. You can print default and current selections out here also.
6. For each camera, pick the camera name from the dropdown menu, and then browse for the associated .yaml config path. The world camera needs to be listed here, even though DLC won't do pose estimation on it. Leave the DLC options empty for the world camera.You can choose to crop videos before running DLC, and you can also choose to treat the top camera as a multianimal project.
7. If you want to get .nc files out, start by checking the 'get parameters' box in the top right-hand corner. With the current eye network that we use, you'll need to check the box on the left side labeled 'is the tear duct labeled in this eye network.' All the eye, top, and misc. options on the left side of the window are pretty standard and don't need to be changed (aside from that one already mentioned which manages the tear duct points). Information about what each of the paramters means is described below in the 'Functionality' section of this file, but the labels in the GUI interface should be pretty clear too. The right side of that page is more about run options than data management. There are execution options, save options, and cameras. Eye rotation is quite slow to run, so it's an optional step. Topdown head and body angles aren't going to be useful unless the tracking in the top view is consistant, so it's optional as well. We usually resize videos before storing them in the .nc file by a factor of 0.5, and this can be changed. You should **always** choose to save out diagnostic figures (this is admittedly a silly thing to make optional). Saving out diagnostic .avi video is a really good idea, and you can choose how many frames to save out. Saving 3600 frames is usually enough (that's one minute), but doing more won't take up too much time. Saving videos in .nc files will let you run later analysis, so you should chech this box too. Lastly, you need to select all the cameras you want to run preprocessing on. This should match pose estimation cameras exactly, if you're running that step too. The order of cameras doesn't matter, just that the same cameras are present in both tabs.
8. If you want to track an IR LED for eye-world calibration, that happens on this tab. Select the box to track IR LEDs, and then browse for the world and eye IR LED DLC .yaml config files (these are **not** the regular DLC networks, these are specificlly for tracking LEDs in a dark room). You also need to enter the directory name for the LED recording (e.g. `hf3_IRspot`) and the likelihood threshold to use, which should be more strict than the one used for tracking mouse points (we don't need to include all points for this analysis, it's more important to include best points).
9. Now you can click the button to write the config. The config file that's written will be printed in the terminal as a dictionary. If it doesn't look correct, you can go back and change things without restarting the GUI before clicking 'write' again.
10. Now you can click this to read the .json config that was writen to file and run all of the preprocessing. You need to keep the GUI window open while the code runs. You'll get progress updates in the terminal.

### Using a .csv for batch preprocessing
1. Write a csv file with each index representing a single day+mouse session with the following required columns shown in the table below, where 'run_preproc' is True if you want preprocessing to be run all recordings of that sesion, where 'run_ephys' is True if you want ephys analysis to be run on all recordings of that session, where 'load_for_data_pool' is whether or not to pool the data for pooled population analysis (this would be run later, with the function `load_ephys` of `project_analysis.ephys.ephys_utils`), where unit2highlight is the unit to map audio for in mp4 videos (the index of the unit in goodcells, not the index in the raw ephys data).

| run_preproc | run_ephys | load_for_data_pool | rec_types | unit2highlight | Data location (i.e. V2/Kraken, drive) |
|---|---|---|---|---|---|
| `True`| `True`| `True` | `date_subject_exp_rig_fm1, date_subject_exp_rig_hf1_wn`| `0` | `/path/to/animal/directory`|

2. Run `python batch_analysis.py --csv_filepath /path/to/csv/file/experiment_pool.csv --log_dir /path/to/save/directory/for/log/csv/ --clear_dlc True`, where `clear_dlc` deletes DeepLabCut .h5 files from all networks, which can be necessary if the name of the network being used changes (i.e. if one camera ends up having more than one .h5 file).